{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/danish/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/danish/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, svm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My Cozy Glam Aaliyah inspired OOTD aaliyahinsp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What a guy   #chubbygirlsdoitbetter beyourself...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beautiful disaster</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Part Mom</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Depressed piece of shit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>math teacher calls irrational numbers female n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How did you manage to blow through a diaper AN...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Thick thighs Thin patience</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i once got told by my boss that i was hired be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a man stood next to me leant close in over my ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>being patronised by being called a girl in eve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Mind blowing hypocrisy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>i was 5 when i was raped and forced into child...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>at a festival my friend politely asked a rando...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>girls mean gay</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Rappel  de</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Girl you are We call it thick or nothing wrong...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Fat shaming For</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>It took tries and back and forths to fit into ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>YOUR ABUSE RECOVERY NarcissiticAbuse   narciss...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Facts let this cruel world take that from you</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Pool ready fatkiniseason</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>as she never answers anymore hates this</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>told by mum when i was younger and had her bru...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>YOUR ABUSE RECOVERY NarcissiticAbuse   narciss...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>They are laughing at Not with You</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>I am offended via this meme so spreading aware...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Bonequinhas dormindo  . . . . . . . . . #bebef...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Repost the difference metoo metoomilitary spea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Morning mirror pep talks</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5968</th>\n",
       "      <td>Three young men were yelling out of their car ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5969</th>\n",
       "      <td>This from a father who called me a fatass and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5970</th>\n",
       "      <td>Everyone absorbs the myth that males to some c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5971</th>\n",
       "      <td>During development of my Supposedly harmless c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5972</th>\n",
       "      <td>some people tell me that im too skinny and i h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5973</th>\n",
       "      <td>Im not a pretty girl and Im a little fat and m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5974</th>\n",
       "      <td>The first message I ever got on FaceBook was Y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5975</th>\n",
       "      <td>As a somewhat masculine looking woman I am oft...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5976</th>\n",
       "      <td>In the wake of all the blatant fatshaming that...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5977</th>\n",
       "      <td>I overheard my old boss saying that my (perfec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5978</th>\n",
       "      <td>My uncle has always been jealous and resentful...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5979</th>\n",
       "      <td>was one told by a friend that I wasnt pretty e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5980</th>\n",
       "      <td>I stopped reading the creatively funny memes p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5981</th>\n",
       "      <td>One of my friends is on the phone trying to bo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5982</th>\n",
       "      <td>A new man started working in our office. He co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5983</th>\n",
       "      <td>I was talking to a pretty average guy (in ever...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5984</th>\n",
       "      <td>Being shouted at as I walked last two 11-12yo ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5985</th>\n",
       "      <td>Walking down the street with my boyfriend. Thr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5986</th>\n",
       "      <td>One day it was very hot so I was wearing short...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5987</th>\n",
       "      <td>Today going to be posting about a topic which ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5988</th>\n",
       "      <td>Had my first dose of cyberbullyng last They st...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5989</th>\n",
       "      <td>I was lying in bed and heard my female apartme...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5990</th>\n",
       "      <td>I going to make this post but I feel like I ne...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5991</th>\n",
       "      <td>Today going to be posting about a topic which ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5992</th>\n",
       "      <td>Very recently someone who I liked has been mak...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5993</th>\n",
       "      <td>Saw this She was bigger than I thought be fats...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5994</th>\n",
       "      <td>Ever since i have had harassing commentaries r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>let me introduce im girl who has a dark unskin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>I had a breast reduction and was repeatedly ba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>I was finally feeling good in my own body afte...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5998 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     My Cozy Glam Aaliyah inspired OOTD aaliyahinsp...      0\n",
       "1     What a guy   #chubbygirlsdoitbetter beyourself...      1\n",
       "2                                    Beautiful disaster      0\n",
       "3                                              Part Mom      0\n",
       "4                               Depressed piece of shit      0\n",
       "5     math teacher calls irrational numbers female n...      0\n",
       "6     How did you manage to blow through a diaper AN...      1\n",
       "7                            Thick thighs Thin patience      1\n",
       "8     i once got told by my boss that i was hired be...      1\n",
       "9     a man stood next to me leant close in over my ...      1\n",
       "10    being patronised by being called a girl in eve...      1\n",
       "11                               Mind blowing hypocrisy      1\n",
       "12    i was 5 when i was raped and forced into child...      1\n",
       "13    at a festival my friend politely asked a rando...      1\n",
       "14                                       girls mean gay      1\n",
       "15                                           Rappel  de      0\n",
       "16    Girl you are We call it thick or nothing wrong...      0\n",
       "17                                      Fat shaming For      0\n",
       "18    It took tries and back and forths to fit into ...      0\n",
       "19    YOUR ABUSE RECOVERY NarcissiticAbuse   narciss...      0\n",
       "20        Facts let this cruel world take that from you      0\n",
       "21                             Pool ready fatkiniseason      0\n",
       "22              as she never answers anymore hates this      0\n",
       "23    told by mum when i was younger and had her bru...      0\n",
       "24    YOUR ABUSE RECOVERY NarcissiticAbuse   narciss...      0\n",
       "25                    They are laughing at Not with You      0\n",
       "26    I am offended via this meme so spreading aware...      0\n",
       "27    Bonequinhas dormindo  . . . . . . . . . #bebef...      0\n",
       "28    Repost the difference metoo metoomilitary spea...      0\n",
       "29                             Morning mirror pep talks      0\n",
       "...                                                 ...    ...\n",
       "5968  Three young men were yelling out of their car ...      1\n",
       "5969  This from a father who called me a fatass and ...      1\n",
       "5970  Everyone absorbs the myth that males to some c...      0\n",
       "5971  During development of my Supposedly harmless c...      1\n",
       "5972  some people tell me that im too skinny and i h...      1\n",
       "5973  Im not a pretty girl and Im a little fat and m...      1\n",
       "5974  The first message I ever got on FaceBook was Y...      1\n",
       "5975  As a somewhat masculine looking woman I am oft...      1\n",
       "5976  In the wake of all the blatant fatshaming that...      1\n",
       "5977  I overheard my old boss saying that my (perfec...      1\n",
       "5978  My uncle has always been jealous and resentful...      1\n",
       "5979  was one told by a friend that I wasnt pretty e...      1\n",
       "5980  I stopped reading the creatively funny memes p...      1\n",
       "5981  One of my friends is on the phone trying to bo...      1\n",
       "5982  A new man started working in our office. He co...      1\n",
       "5983  I was talking to a pretty average guy (in ever...      1\n",
       "5984  Being shouted at as I walked last two 11-12yo ...      1\n",
       "5985  Walking down the street with my boyfriend. Thr...      1\n",
       "5986  One day it was very hot so I was wearing short...      1\n",
       "5987  Today going to be posting about a topic which ...      1\n",
       "5988  Had my first dose of cyberbullyng last They st...      1\n",
       "5989  I was lying in bed and heard my female apartme...      1\n",
       "5990  I going to make this post but I feel like I ne...      1\n",
       "5991  Today going to be posting about a topic which ...      0\n",
       "5992  Very recently someone who I liked has been mak...      1\n",
       "5993  Saw this She was bigger than I thought be fats...      1\n",
       "5994  Ever since i have had harassing commentaries r...      1\n",
       "5995  let me introduce im girl who has a dark unskin...      1\n",
       "5996  I had a breast reduction and was repeatedly ba...      1\n",
       "5997  I was finally feeling good in my own body afte...      1\n",
       "\n",
       "[5998 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset/data.txt\", delimiter=',',encoding = \"utf-8\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = [entry.lower() for entry in df['text']]\n",
    "df['text']= [word_tokenize(entry) for entry in df['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['cozy', 'glam', 'aaliyah', 'inspire', 'ootd',...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['guy', 'chubbygirlsdoitbetter', 'beyourself',...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['beautiful', 'disaster']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['part', 'mom']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['depressed', 'piece', 'shit']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  ['cozy', 'glam', 'aaliyah', 'inspire', 'ootd',...      0\n",
       "1  ['guy', 'chubbygirlsdoitbetter', 'beyourself',...      1\n",
       "2                          ['beautiful', 'disaster']      0\n",
       "3                                    ['part', 'mom']      0\n",
       "4                     ['depressed', 'piece', 'shit']      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Corpus  = df\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "for index,entry in enumerate(Corpus['text']):\n",
    "    Final_words = []\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(entry):\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    Corpus.loc[index,'text'] = str(Final_words)\n",
    "Corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Corpus['text'],Corpus['label'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(analyzer='word',max_features=5000)\n",
    "Tfidf_vect.fit(Corpus['text'])\n",
    "Train_X_Tfidf_word = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf_word = Tfidf_vect.transform(Test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(analyzer='word',ngram_range=(2,3),max_features=5000)\n",
    "Tfidf_vect.fit(Corpus['text'])\n",
    "Train_X_Tfidf_ng = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf_ng = Tfidf_vect.transform(Test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(analyzer='char',ngram_range=(2,3),max_features=5000)\n",
    "Tfidf_vect.fit(Corpus['text'])\n",
    "Train_X_Tfidf_chr = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf_chr = Tfidf_vect.transform(Test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM word,n-gram,char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ->  91.08333333333334\n",
      "F1 Score ->  0.8595762132604239\n",
      "Precision ->  0.8737120895495267\n",
      "Recall ->  0.8473958927191416\n"
     ]
    }
   ],
   "source": [
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf_word,Train_Y)\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf_word)\n",
    "print(\"Accuracy -> \",accuracy_score( Test_Y,predictions_SVM)*100)\n",
    "print(\"F1 Score -> \",f1_score(Test_Y,predictions_SVM, average=\"macro\"))\n",
    "print(\"Precision -> \",precision_score(Test_Y, predictions_SVM, average=\"macro\"))\n",
    "print(\"Recall -> \",recall_score(Test_Y,predictions_SVM, average=\"macro\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ->  88.08333333333334\n",
      "F1 Score ->  0.7948033591981547\n",
      "Precision ->  0.8491799421926464\n",
      "Recall ->  0.763248577907846\n"
     ]
    }
   ],
   "source": [
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf_ng,Train_Y)\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf_ng)\n",
    "print(\"Accuracy -> \",accuracy_score( Test_Y,predictions_SVM)*100)\n",
    "print(\"F1 Score -> \",f1_score(Test_Y,predictions_SVM, average=\"macro\"))\n",
    "print(\"Precision -> \",precision_score(Test_Y, predictions_SVM, average=\"macro\"))\n",
    "print(\"Recall -> \",recall_score(Test_Y,predictions_SVM, average=\"macro\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ->  90.91666666666667\n",
      "F1 Score ->  0.8621124416112604\n",
      "Precision ->  0.861578947368421\n",
      "Recall ->  0.8626493355123965\n"
     ]
    }
   ],
   "source": [
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf_chr,Train_Y)\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf_chr)\n",
    "print(\"Accuracy -> \",accuracy_score( Test_Y,predictions_SVM)*100)\n",
    "print(\"F1 Score -> \",f1_score(Test_Y,predictions_SVM, average=\"macro\"))\n",
    "print(\"Precision -> \",precision_score(Test_Y, predictions_SVM, average=\"macro\"))\n",
    "print(\"Recall -> \",recall_score(Test_Y,predictions_SVM, average=\"macro\"))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest word,char,n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ->  79.83333333333333\n",
      "F1 Score ->  0.4745184895663692\n",
      "Precision ->  0.8432689616568709\n",
      "Recall ->  0.5155384946726971\n"
     ]
    }
   ],
   "source": [
    "RFC = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0)\n",
    "RFC.fit(Train_X_Tfidf_word,Train_Y)\n",
    "predictions_RFC = RFC.predict(Test_X_Tfidf_word)\n",
    "print(\"Accuracy -> \",accuracy_score( Test_Y,predictions_RFC)*100)\n",
    "print(\"F1 Score -> \",f1_score(Test_Y,predictions_RFC, average=\"macro\"))\n",
    "print(\"Precision -> \",precision_score(Test_Y, predictions_RFC, average=\"macro\"))\n",
    "print(\"Recall -> \",recall_score(Test_Y,predictions_RFC, average=\"macro\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ->  82.58333333333333\n",
      "F1 Score ->  0.5984572304115814\n",
      "Precision ->  0.8693576388888888\n",
      "Recall ->  0.5862503642329571\n"
     ]
    }
   ],
   "source": [
    "RFC = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0)\n",
    "RFC.fit(Train_X_Tfidf_chr,Train_Y)\n",
    "predictions_RFC = RFC.predict(Test_X_Tfidf_chr)\n",
    "print(\"Accuracy -> \",accuracy_score( Test_Y,predictions_RFC)*100)\n",
    "print(\"F1 Score -> \",f1_score(Test_Y,predictions_RFC, average=\"macro\"))\n",
    "print(\"Precision -> \",precision_score(Test_Y, predictions_RFC, average=\"macro\"))\n",
    "print(\"Recall -> \",recall_score(Test_Y,predictions_RFC, average=\"macro\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ->  79.25\n",
      "F1 Score ->  0.4421199442119944\n",
      "Precision ->  0.39625\n",
      "Recall ->  0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danish/Anacond/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/danish/Anacond/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "RFC = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0)\n",
    "RFC.fit(Train_X_Tfidf_ng,Train_Y)\n",
    "predictions_RFC = RFC.predict(Test_X_Tfidf_ng)\n",
    "print(\"Accuracy -> \",accuracy_score( Test_Y,predictions_RFC)*100)\n",
    "print(\"F1 Score -> \",f1_score(Test_Y,predictions_RFC, average=\"macro\"))\n",
    "print(\"Precision -> \",precision_score(Test_Y, predictions_RFC, average=\"macro\"))\n",
    "print(\"Recall -> \",recall_score(Test_Y,predictions_RFC, average=\"macro\"))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ->  89.5\n",
      "F1 Score ->  0.8181818181818182\n",
      "Precision ->  0.8802618791877653\n",
      "Recall ->  0.7825624263615978\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression(random_state=0)\n",
    "LR.fit(Train_X_Tfidf_word,Train_Y)\n",
    "predictions_LR = LR.predict(Test_X_Tfidf_word)\n",
    "print(\"Accuracy -> \",accuracy_score(Test_Y,predictions_LR)*100)\n",
    "print(\"F1 Score -> \",f1_score(Test_Y,predictions_LR, average=\"macro\"))\n",
    "print(\"Precision -> \",precision_score(Test_Y, predictions_LR, average=\"macro\"))\n",
    "print(\"Recall -> \",recall_score(Test_Y,predictions_LR, average=\"macro\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ->  89.66666666666666\n",
      "F1 Score ->  0.8359607540513724\n",
      "Precision ->  0.8525875974653239\n",
      "Recall ->  0.8221529651729949\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression(random_state=0)\n",
    "LR.fit(Train_X_Tfidf_chr,Train_Y)\n",
    "predictions_LR = LR.predict(Test_X_Tfidf_chr)\n",
    "print(\"Accuracy -> \",accuracy_score(Test_Y,predictions_LR)*100)\n",
    "print(\"F1 Score -> \",f1_score(Test_Y,predictions_LR, average=\"macro\"))\n",
    "print(\"Precision -> \",precision_score(Test_Y, predictions_LR, average=\"macro\"))\n",
    "print(\"Recall -> \",recall_score(Test_Y,predictions_LR, average=\"macro\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ->  85.08333333333333\n",
      "F1 Score ->  0.6970273987094044\n",
      "Precision ->  0.8571105072463768\n",
      "Recall ->  0.6598317560462671\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression(random_state=0)\n",
    "LR.fit(Train_X_Tfidf_ng,Train_Y)\n",
    "predictions_LR = LR.predict(Test_X_Tfidf_ng)\n",
    "print(\"Accuracy -> \",accuracy_score(Test_Y,predictions_LR)*100)\n",
    "print(\"F1 Score -> \",f1_score(Test_Y,predictions_LR, average=\"macro\"))\n",
    "print(\"Precision -> \",precision_score(Test_Y, predictions_LR, average=\"macro\"))\n",
    "print(\"Recall -> \",recall_score(Test_Y,predictions_LR, average=\"macro\"))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

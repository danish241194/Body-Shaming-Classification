{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "with open('dataset/45k.txt') as f:\n",
    "    all_data = f.readlines()\n",
    "final_data = []\n",
    "count=0\n",
    "for data in all_data:\n",
    "    count+=1\n",
    "    final_data.append(data.strip())\n",
    "lst = final_data\n",
    "df = pd.DataFrame(lst[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['index'] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing\n",
    "\n",
    "We will perform the following steps:\n",
    "\n",
    "- Tokenization: Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation.\n",
    "- Words that have fewer than 3 characters are removed.\n",
    "- All stopwords are removed.\n",
    "- Words are lemmatized — words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
    "- Words are stemmed — words are reduced to their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents[0].map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words on the Data set\n",
    "\n",
    "- Create a dictionary from ‘processed_docs’ containing the number of times a word appears in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out tokens that appear in\n",
    "\n",
    "- less than 15 documents (absolute number) or\n",
    "- more than 0.5 documents (fraction of total corpus size, not absolute number).\n",
    "- after the above two steps, keep only the first 100000 most frequent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim doc2bow\n",
    "\n",
    "For each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to ‘bow_corpus’, then check our selected document earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "Create tf-idf model object using models.TfidfModel on ‘bow_corpus’ and save it to ‘tfidf’, then apply transformation to the entire corpus and call it ‘corpus_tfidf’. Finally we preview TF-IDF scores for our first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running LDA using Bag of Words\n",
    "- Train our lda model using gensim.models.LdaMulticore and save it to ‘lda_model’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda_topics(model, num_topics):\n",
    "    word_dict = {};\n",
    "    for i in range(num_topics):\n",
    "        words = model.show_topic(i, topn = 20);\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words];\n",
    "    return pd.DataFrame(word_dict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=5, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "will explore the words occuring in that topic and its relative weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic # 01</th>\n",
       "      <th>Topic # 02</th>\n",
       "      <th>Topic # 03</th>\n",
       "      <th>Topic # 04</th>\n",
       "      <th>Topic # 05</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>bodi</td>\n",
       "      <td>bodyposit</td>\n",
       "      <td>bodi</td>\n",
       "      <td>bodyposit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>metoo</td>\n",
       "      <td>love</td>\n",
       "      <td>plussiz</td>\n",
       "      <td>feel</td>\n",
       "      <td>abus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fatkini</td>\n",
       "      <td>know</td>\n",
       "      <td>plussizefashion</td>\n",
       "      <td>like</td>\n",
       "      <td>loveyourself</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metoomov</td>\n",
       "      <td>like</td>\n",
       "      <td>metoo</td>\n",
       "      <td>look</td>\n",
       "      <td>honormycurv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>happi</td>\n",
       "      <td>want</td>\n",
       "      <td>loveyourself</td>\n",
       "      <td>think</td>\n",
       "      <td>survivor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>like</td>\n",
       "      <td>work</td>\n",
       "      <td>beauti</td>\n",
       "      <td>peopl</td>\n",
       "      <td>share</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>thank</td>\n",
       "      <td>peopl</td>\n",
       "      <td>women</td>\n",
       "      <td>love</td>\n",
       "      <td>sexual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>eboni</td>\n",
       "      <td>weight</td>\n",
       "      <td>love</td>\n",
       "      <td>want</td>\n",
       "      <td>metoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bodyposit</td>\n",
       "      <td>time</td>\n",
       "      <td>celebratemys</td>\n",
       "      <td>time</td>\n",
       "      <td>stori</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>time</td>\n",
       "      <td>feel</td>\n",
       "      <td>selflov</td>\n",
       "      <td>know</td>\n",
       "      <td>timesup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>need</td>\n",
       "      <td>share</td>\n",
       "      <td>like</td>\n",
       "      <td>need</td>\n",
       "      <td>selflov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>kirk</td>\n",
       "      <td>thing</td>\n",
       "      <td>plussizemodel</td>\n",
       "      <td>tell</td>\n",
       "      <td>babi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>want</td>\n",
       "      <td>help</td>\n",
       "      <td>ministri</td>\n",
       "      <td>thing</td>\n",
       "      <td>want</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>believewomen</td>\n",
       "      <td>need</td>\n",
       "      <td>curvygirl</td>\n",
       "      <td>good</td>\n",
       "      <td>anonym</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>power</td>\n",
       "      <td>lose</td>\n",
       "      <td>curvi</td>\n",
       "      <td>beauti</td>\n",
       "      <td>metoomov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>workout</td>\n",
       "      <td>go</td>\n",
       "      <td>follow</td>\n",
       "      <td>go</td>\n",
       "      <td>endrap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>summer</td>\n",
       "      <td>post</td>\n",
       "      <td>bodi</td>\n",
       "      <td>wear</td>\n",
       "      <td>itsnotyourfault</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rapper</td>\n",
       "      <td>life</td>\n",
       "      <td>fashion</td>\n",
       "      <td>posit</td>\n",
       "      <td>take</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>littl</td>\n",
       "      <td>come</td>\n",
       "      <td>fatshion</td>\n",
       "      <td>come</td>\n",
       "      <td>endrapecultur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>feminist</td>\n",
       "      <td>think</td>\n",
       "      <td>model</td>\n",
       "      <td>start</td>\n",
       "      <td>number</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Topic # 01 Topic # 02       Topic # 03 Topic # 04       Topic # 05\n",
       "0           love       bodi        bodyposit       bodi        bodyposit\n",
       "1          metoo       love          plussiz       feel             abus\n",
       "2        fatkini       know  plussizefashion       like     loveyourself\n",
       "3       metoomov       like            metoo       look      honormycurv\n",
       "4          happi       want     loveyourself      think         survivor\n",
       "5           like       work           beauti      peopl            share\n",
       "6          thank      peopl            women       love           sexual\n",
       "7          eboni     weight             love       want            metoo\n",
       "8      bodyposit       time     celebratemys       time            stori\n",
       "9           time       feel          selflov       know          timesup\n",
       "10          need      share             like       need          selflov\n",
       "11          kirk      thing    plussizemodel       tell             babi\n",
       "12          want       help         ministri      thing             want\n",
       "13  believewomen       need        curvygirl       good           anonym\n",
       "14         power       lose            curvi     beauti         metoomov\n",
       "15       workout         go           follow         go           endrap\n",
       "16        summer       post             bodi       wear  itsnotyourfault\n",
       "17        rapper       life          fashion      posit             take\n",
       "18         littl       come         fatshion       come    endrapecultur\n",
       "19      feminist      think            model      start           number"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lda_topics(lda_model,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=5, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic # 01</th>\n",
       "      <th>Topic # 02</th>\n",
       "      <th>Topic # 03</th>\n",
       "      <th>Topic # 04</th>\n",
       "      <th>Topic # 05</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>share</td>\n",
       "      <td>love</td>\n",
       "      <td>bodi</td>\n",
       "      <td>repost</td>\n",
       "      <td>bodyposit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stori</td>\n",
       "      <td>thank</td>\n",
       "      <td>love</td>\n",
       "      <td>bodyposit</td>\n",
       "      <td>loveyourself</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bodi</td>\n",
       "      <td>braveri</td>\n",
       "      <td>like</td>\n",
       "      <td>celebratemys</td>\n",
       "      <td>selflov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>want</td>\n",
       "      <td>work</td>\n",
       "      <td>feel</td>\n",
       "      <td>metoo</td>\n",
       "      <td>plussiz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>metoo</td>\n",
       "      <td>like</td>\n",
       "      <td>look</td>\n",
       "      <td>plussiz</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>need</td>\n",
       "      <td>great</td>\n",
       "      <td>peopl</td>\n",
       "      <td>metoomov</td>\n",
       "      <td>follow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>know</td>\n",
       "      <td>feel</td>\n",
       "      <td>think</td>\n",
       "      <td>honormycurv</td>\n",
       "      <td>plussizefashion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>thank</td>\n",
       "      <td>time</td>\n",
       "      <td>thing</td>\n",
       "      <td>plussizefashion</td>\n",
       "      <td>curvi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>time</td>\n",
       "      <td>life</td>\n",
       "      <td>posit</td>\n",
       "      <td>bopo</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>believewomen</td>\n",
       "      <td>know</td>\n",
       "      <td>want</td>\n",
       "      <td>love</td>\n",
       "      <td>curvygirl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>feel</td>\n",
       "      <td>week</td>\n",
       "      <td>beauti</td>\n",
       "      <td>imnomodeleith</td>\n",
       "      <td>plussizemodel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>take</td>\n",
       "      <td>amaz</td>\n",
       "      <td>know</td>\n",
       "      <td>bodi</td>\n",
       "      <td>plusisequ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>timesup</td>\n",
       "      <td>bodi</td>\n",
       "      <td>women</td>\n",
       "      <td>sexual</td>\n",
       "      <td>beauti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>abus</td>\n",
       "      <td>wait</td>\n",
       "      <td>need</td>\n",
       "      <td>link</td>\n",
       "      <td>honormycurv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>survivor</td>\n",
       "      <td>share</td>\n",
       "      <td>good</td>\n",
       "      <td>beauti</td>\n",
       "      <td>newbi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>enoughisenough</td>\n",
       "      <td>weekend</td>\n",
       "      <td>happi</td>\n",
       "      <td>fatbab</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>voic</td>\n",
       "      <td>start</td>\n",
       "      <td>tell</td>\n",
       "      <td>dress</td>\n",
       "      <td>gorgeous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>love</td>\n",
       "      <td>readi</td>\n",
       "      <td>work</td>\n",
       "      <td>selfi</td>\n",
       "      <td>plussizebeauti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>strength</td>\n",
       "      <td>miss</td>\n",
       "      <td>chang</td>\n",
       "      <td>selflov</td>\n",
       "      <td>tattoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>endrap</td>\n",
       "      <td>go</td>\n",
       "      <td>go</td>\n",
       "      <td>women</td>\n",
       "      <td>girl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Topic # 01 Topic # 02 Topic # 03       Topic # 04       Topic # 05\n",
       "0            share       love       bodi           repost        bodyposit\n",
       "1            stori      thank       love        bodyposit     loveyourself\n",
       "2             bodi    braveri       like     celebratemys          selflov\n",
       "3             want       work       feel            metoo          plussiz\n",
       "4            metoo       like       look          plussiz            model\n",
       "5             need      great      peopl         metoomov           follow\n",
       "6             know       feel      think      honormycurv  plussizefashion\n",
       "7            thank       time      thing  plussizefashion            curvi\n",
       "8             time       life      posit             bopo             love\n",
       "9     believewomen       know       want             love        curvygirl\n",
       "10            feel       week     beauti    imnomodeleith    plussizemodel\n",
       "11            take       amaz       know             bodi        plusisequ\n",
       "12         timesup       bodi      women           sexual           beauti\n",
       "13            abus       wait       need             link      honormycurv\n",
       "14        survivor      share       good           beauti            newbi\n",
       "15  enoughisenough    weekend      happi           fatbab             like\n",
       "16            voic      start       tell            dress         gorgeous\n",
       "17            love      readi       work            selfi   plussizebeauti\n",
       "18        strength       miss      chang          selflov           tattoo\n",
       "19          endrap         go         go            women             girl"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lda_topics(lda_model_tfidf,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    " \n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    " \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "from sklearn.decomposition import NMF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= lst[1:]\n",
    "vectorizer = TfidfVectorizer(max_features=20000, min_df=10, stop_words='english')\n",
    "X = vectorizer.fit_transform(data)\n",
    "idx_to_word = np.array(vectorizer.get_feature_names())\n",
    "nmf = NMF(n_components=5, solver=\"mu\")\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nmf_topics(model, n_top_words):\n",
    "    \n",
    "    #the word ids obtained need to be reverse-mapped to the words so we can print the topic names.\n",
    "    feat_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    word_dict = {};\n",
    "    for i in range(5):\n",
    "        \n",
    "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
    "        words_ids = model.components_[i].argsort()[:-20 - 1:-1]\n",
    "        words = [feat_names[key] for key in words_ids]\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words;\n",
    "    \n",
    "    return pd.DataFrame(word_dict).iloc[:n_top_words,:n_top_words];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic # 01</th>\n",
       "      <th>Topic # 02</th>\n",
       "      <th>Topic # 03</th>\n",
       "      <th>Topic # 04</th>\n",
       "      <th>Topic # 05</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>body</td>\n",
       "      <td>story</td>\n",
       "      <td>follow</td>\n",
       "      <td>ministries</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>just</td>\n",
       "      <td>believewomen</td>\n",
       "      <td>gorgeous</td>\n",
       "      <td>kirk</td>\n",
       "      <td>bodypositive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>like</td>\n",
       "      <td>believesurvivors</td>\n",
       "      <td>model</td>\n",
       "      <td>franklin</td>\n",
       "      <td>bodypositivity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feel</td>\n",
       "      <td>metoomeredith</td>\n",
       "      <td>like</td>\n",
       "      <td>justin</td>\n",
       "      <td>selflove</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>people</td>\n",
       "      <td>enoughisenough</td>\n",
       "      <td>explorepage</td>\n",
       "      <td>ebony</td>\n",
       "      <td>effyourbeautystandards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>know</td>\n",
       "      <td>whyididntreport</td>\n",
       "      <td>metoochallenge</td>\n",
       "      <td>jimmy</td>\n",
       "      <td>loveyourself</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>make</td>\n",
       "      <td>anonymously</td>\n",
       "      <td>explore</td>\n",
       "      <td>perry</td>\n",
       "      <td>beautiful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>time</td>\n",
       "      <td>iamasurvivor</td>\n",
       "      <td>blueface</td>\n",
       "      <td>steve</td>\n",
       "      <td>plussize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>want</td>\n",
       "      <td>itsnotyourfault</td>\n",
       "      <td>laiiandnayah</td>\n",
       "      <td>gomez</td>\n",
       "      <td>plussizefashion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>look</td>\n",
       "      <td>endrape</td>\n",
       "      <td>ynwmelly</td>\n",
       "      <td>bruno</td>\n",
       "      <td>bodyshaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>way</td>\n",
       "      <td>keepfighting</td>\n",
       "      <td>thotianachallenge</td>\n",
       "      <td>ariana</td>\n",
       "      <td>honormycurves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>need</td>\n",
       "      <td>endrapeculture</td>\n",
       "      <td>bustdownthotiana</td>\n",
       "      <td>rick</td>\n",
       "      <td>self</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>think</td>\n",
       "      <td>toxicrelationships</td>\n",
       "      <td>newchallenge</td>\n",
       "      <td>joyce</td>\n",
       "      <td>bodypositivitymovement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>day</td>\n",
       "      <td>familyviolence</td>\n",
       "      <td>alohaahyeschallenge</td>\n",
       "      <td>grande</td>\n",
       "      <td>celebratemysize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>good</td>\n",
       "      <td>yourvoicematters</td>\n",
       "      <td>pieceofyolovechallenge</td>\n",
       "      <td>jennifer</td>\n",
       "      <td>loving</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>women</td>\n",
       "      <td>youdeservebetter</td>\n",
       "      <td>bodypositive</td>\n",
       "      <td>degeneres</td>\n",
       "      <td>bopo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>things</td>\n",
       "      <td>rapeculture</td>\n",
       "      <td>d1xsike</td>\n",
       "      <td>minaj</td>\n",
       "      <td>plussizemodel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>really</td>\n",
       "      <td>metoo</td>\n",
       "      <td>royaltysquad</td>\n",
       "      <td>radiofree</td>\n",
       "      <td>curvy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>life</td>\n",
       "      <td>childabuse</td>\n",
       "      <td>shoota</td>\n",
       "      <td>meyer</td>\n",
       "      <td>bodypositivemovement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>going</td>\n",
       "      <td>abuseawareness</td>\n",
       "      <td>eishavsnyema</td>\n",
       "      <td>coldplay</td>\n",
       "      <td>curvygirl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic # 01          Topic # 02              Topic # 03  Topic # 04  \\\n",
       "0        body               story                  follow  ministries   \n",
       "1        just        believewomen                gorgeous        kirk   \n",
       "2        like    believesurvivors                   model    franklin   \n",
       "3        feel       metoomeredith                    like      justin   \n",
       "4      people      enoughisenough             explorepage       ebony   \n",
       "5        know     whyididntreport          metoochallenge       jimmy   \n",
       "6        make         anonymously                 explore       perry   \n",
       "7        time        iamasurvivor                blueface       steve   \n",
       "8        want     itsnotyourfault            laiiandnayah       gomez   \n",
       "9        look             endrape                ynwmelly       bruno   \n",
       "10        way        keepfighting       thotianachallenge      ariana   \n",
       "11       need      endrapeculture        bustdownthotiana        rick   \n",
       "12      think  toxicrelationships            newchallenge       joyce   \n",
       "13        day      familyviolence     alohaahyeschallenge      grande   \n",
       "14       good    yourvoicematters  pieceofyolovechallenge    jennifer   \n",
       "15      women    youdeservebetter            bodypositive   degeneres   \n",
       "16     things         rapeculture                 d1xsike       minaj   \n",
       "17     really               metoo            royaltysquad   radiofree   \n",
       "18       life          childabuse                  shoota       meyer   \n",
       "19      going      abuseawareness            eishavsnyema    coldplay   \n",
       "\n",
       "                Topic # 05  \n",
       "0                     love  \n",
       "1             bodypositive  \n",
       "2           bodypositivity  \n",
       "3                 selflove  \n",
       "4   effyourbeautystandards  \n",
       "5             loveyourself  \n",
       "6                beautiful  \n",
       "7                 plussize  \n",
       "8          plussizefashion  \n",
       "9              bodyshaming  \n",
       "10           honormycurves  \n",
       "11                    self  \n",
       "12  bodypositivitymovement  \n",
       "13         celebratemysize  \n",
       "14                  loving  \n",
       "15                    bopo  \n",
       "16           plussizemodel  \n",
       "17                   curvy  \n",
       "18    bodypositivemovement  \n",
       "19               curvygirl  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nmf_topics(nmf,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
